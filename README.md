# 🔐 DarkBERT Fill-Mask Demo

This is an interactive demo using [`s2w-ai/DarkBERT`](https://huggingface.co/s2w-ai/DarkBERT), a BERT-based language model pre-trained on dark web content. The model is designed for research and academic use, particularly in cybersecurity, threat intelligence, and forensic linguistics.

---

## 🚀 How to Use

1. Enter a sentence that includes `[MASK]`, for example:
2. The model will return the top-5 predicted words to fill in the masked token.

---

## 🔒 Access Notice

This model is hosted under a gated license (`cc-by-nc-4.0`) and is restricted to **non-commercial, research use only**. You must be granted access by [s2w-ai](https://huggingface.co/s2w-ai/DarkBERT) to use this model.

---

## 📚 Citation

If you use this model in your research, please cite the original paper:

**APA Format:**
> Jin, Y., Jang, E., Cui, J., Chung, J.-W., Lee, Y., & Shin, S. (2023).  
> *DarkBERT: A Language Model for the Dark Side of the Internet*.  
> Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 7515–7533. Toronto, Canada. Association for Computational Linguistics.  
> [Link to paper](https://aclanthology.org/2023.acl-long.419)

**BibTeX:**
```bibtex
@inproceedings{jin-etal-2023-darkbert,
title = "Dark{BERT}: A Language Model for the Dark Side of the Internet",
author = "Jin, Youngjin and Jang, Eugene and Cui, Jian and Chung, Jin-Woo and Lee, Yongjae and Shin, Seungwon",
booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
month = jul,
year = "2023",
address = "Toronto, Canada",
publisher = "Association for Computational Linguistics",
pages = "7515--7533",
url = "https://aclanthology.org/2023.acl-long.419"
}
